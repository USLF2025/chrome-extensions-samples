{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USLF2025/chrome-extensions-samples/blob/main/colabs/sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "My2AZpV_RuTs"
      },
      "cell_type": "markdown",
      "source": [
        "# Sampling\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/gemma/blob/main/colabs/sampling.ipynb)\n",
        "\n",
        "Example on how to load a Gemma model and run inference on it.\n",
        "\n",
        "The Gemma library has 3 ways to prompt a model:\n",
        "\n",
        "* `gm.text.ChatSampler`: Easiest to use, simply talk to the model and get answer. Support multi-turns conversations out-of-the-box.\n",
        "* `gm.text.Sampler`: Lower level, but give more control. The chat state has to be manually handeled for multi-turn.\n",
        "* `model.apply`: Directly call the model, only predict a single token."
      ]
    },
    {
      "metadata": {
        "id": "94CVV9ZxKVDO",
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gemma"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8VZFU0QF47OP"
      }
    },
    {
      "metadata": {
        "id": "PXBc1hRKRuTt"
      },
      "cell_type": "code",
      "source": [
        "# Common imports\n",
        "import os\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Gemma imports\n",
        "from gemma import gm"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "i_DEVehe3v5Y"
      },
      "cell_type": "markdown",
      "source": [
        "By default, Jax do not utilize the full GPU memory, but this can be overwritten. See [GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html):"
      ]
    },
    {
      "metadata": {
        "id": "AaK17GWo3v5Y"
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "XzEj3PYvX1Sm"
      },
      "cell_type": "markdown",
      "source": [
        "Load the model and the params. Here we load the instruction-tuned version of the model."
      ]
    },
    {
      "metadata": {
        "id": "ox1CAuffKJtj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "75206352-2be0-4db1-faac-37a810d1abb6"
      },
      "cell_type": "code",
      "source": [
        "model = gm.nn.Gemma3_4B()\n",
        "\n",
        "params = gm.ckpts.load_params(str(gm.ckpts.CheckpointPath.GEMMA3_4B_IT))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Provided metadata contains unknown key custom. Adding it to custom_metadata.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'StepMetadata' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2376935913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemma3_4B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGEMMA3_4B_IT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gemma/gm/ckpts/_checkpoint.py\u001b[0m in \u001b[0;36mload_params\u001b[0;34m(path, params, donate, text_only, sharding, quantize)\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mocp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardCheckpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m   \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_metadata_and_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CheckpointTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_dtype_struct_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gemma/gm/ckpts/_checkpoint.py\u001b[0m in \u001b[0;36m_get_metadata_and_path\u001b[0;34m(ckpt, path)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'StepMetadata' object is not iterable"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "38f025ac",
        "outputId": "b73bec02-8e1a-498a-94e4-1daf74e423c3"
      },
      "source": [
        "from gemma import gm\n",
        "\n",
        "# Configure and load the Gemma 7B instruction-tuned model using the corrected import\n",
        "model = gm.GemmaForCausalLM.from_pretrained('gemma-7b-it')\n",
        "\n",
        "# The model and parameters should now be loaded and accessible through the 'model' object\n",
        "# You can now use the 'model' for inference."
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'gemma.gm' has no attribute 'GemmaForCausalLM'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3061752261.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Configure and load the Gemma 7B instruction-tuned model using the corrected import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemmaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemma-7b-it'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# The model and parameters should now be loaded and accessible through the 'model' object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/etils/epy/lazy_api_imports_utils.py\u001b[0m in \u001b[0;36m_getattr\u001b[0;34m(name, module_name, imported_symbols, error_msg)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Module `__getattr__` that lazy-imports symbols.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimported_symbols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     raise AttributeError(\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;34mf'module {module_name!r} has no attribute {name!r}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gemma.gm' has no attribute 'GemmaForCausalLM'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c5eadee",
        "outputId": "54d2a60f-dcf9-4d75-847d-ffb0c1d8ee09"
      },
      "source": [
        "from gemma import gm\n",
        "print(dir(gm))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__builtins__', '__cached__', '__dir__', '__doc__', '__file__', '__getattr__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_epy', 'ckpts', 'ckpts', 'data', 'data', 'evals', 'losses', 'math', 'math', 'nn', 'nn', 'peft', 'sharding', 'testing', 'text', 'text', 'tools', 'typing', 'typing', 'utils', 'vision']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "810136f1",
        "outputId": "cc4fe142-fea6-4123-d7bb-03ae53e7d1dc"
      },
      "source": [
        "import gemma\n",
        "\n",
        "# Configure and load the Gemma 7B instruction-tuned model\n",
        "model = gemma.GemmaForCausalLM.from_pretrained('gemma-7b-it')\n",
        "\n",
        "# The model and parameters should now be loaded and accessible through the 'model' object\n",
        "# You can now use the 'model' for inference."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Please use \"from gemma import gm\", NOT \"import gemma as gm\".",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2049538091.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Configure and load the Gemma 7B instruction-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemmaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemma-7b-it'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# The model and parameters should now be loaded and accessible through the 'model' object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gemma/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;34m\"\"\"Catches `import gemma as gm` errors.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   raise AttributeError(\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0;34m'Please use \"from gemma import gm\", NOT \"import gemma as gm\".'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   )\n",
            "\u001b[0;31mAttributeError\u001b[0m: Please use \"from gemma import gm\", NOT \"import gemma as gm\"."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ae7ee7b",
        "outputId": "203a1ac6-291e-4c47-d78e-ad535613699d"
      },
      "source": [
        "company_info_text = \"\"\"COMPANY NAME : US LOGISTICS AND FREIGT FORWARDING LLC. 96063 ESTATE DRIVE YULEE, FL 32097- CORPORATE HQ  ; 260 PEACHTREE STEET ATLANTA, GA  SUITE 2109 30303 AND I AM THE FOUNDER ANTHONY RODRIGUEZ WITH STEPHANIE RODRIGUEZ IS PRINCIPLE OWNER 60-40 SPLIT WOMEN OWNED MINORITY COMPANY\"\"\"\n",
        "\n",
        "# Extracting information (simplified - regex or NLP would be used in a real scenario)\n",
        "company_name = \"US LOGISTICS AND FREIGT FORWARDING LLC.\"\n",
        "corporate_hq_address = \"96063 ESTATE DRIVE YULEE, FL 32097\"\n",
        "atlanta_address = \"260 PEACHTREE STEET ATLANTA, GA SUITE 2109 30303\"\n",
        "founder_name = \"ANTHONY RODRIGUEZ\"\n",
        "principal_owner_name = \"STEPHANIE RODRIGUEZ\"\n",
        "ownership_split = \"60-40\"\n",
        "company_type = \"WOMEN OWNED MINORITY COMPANY\"\n",
        "\n",
        "structured_company_data = {\n",
        "    \"company_name\": company_name,\n",
        "    \"addresses\": {\n",
        "        \"corporate_hq\": corporate_hq_address,\n",
        "        \"atlanta_office\": atlanta_address\n",
        "    },\n",
        "    \"key_personnel\": {\n",
        "        \"founder\": founder_name,\n",
        "        \"principal_owner\": principal_owner_name\n",
        "    },\n",
        "    \"ownership\": {\n",
        "        \"split\": ownership_split,\n",
        "        \"type\": company_type\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "\n",
        "# Convert the dictionary to a JSON string\n",
        "structured_company_data_json = json.dumps(structured_company_data, indent=4)\n",
        "\n",
        "print(structured_company_data_json)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"company_name\": \"US LOGISTICS AND FREIGT FORWARDING LLC.\",\n",
            "    \"addresses\": {\n",
            "        \"corporate_hq\": \"96063 ESTATE DRIVE YULEE, FL 32097\",\n",
            "        \"atlanta_office\": \"260 PEACHTREE STEET ATLANTA, GA SUITE 2109 30303\"\n",
            "    },\n",
            "    \"key_personnel\": {\n",
            "        \"founder\": \"ANTHONY RODRIGUEZ\",\n",
            "        \"principal_owner\": \"STEPHANIE RODRIGUEZ\"\n",
            "    },\n",
            "    \"ownership\": {\n",
            "        \"split\": \"60-40\",\n",
            "        \"type\": \"WOMEN OWNED MINORITY COMPANY\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0607c7d7"
      },
      "source": [
        "### How the Model Receives Data\n",
        "\n",
        "*   **Unstructured Text:** The cleaned unstructured text can be used in several ways:\n",
        "    *   As part of the training data for fine-tuning, allowing the model to learn the language, context, and general information within the document.\n",
        "    *   As input for prompting, where you might ask the model questions directly about the text content.\n",
        "*   **Structured Data:** Structured data provides the model with explicit facts and relationships. This can be used:\n",
        "    *   To create specific training examples (e.g., \"What is the company's key service?\" -> \"US Import/Export Drayage\").\n",
        "    *   In combination with unstructured text, where the model uses the structured data to ground its responses in factual information extracted from the text.\n",
        "    *   For tasks requiring precise information retrieval or analysis.\n",
        "\n",
        "By using both formats, we can leverage the richness of unstructured text and the precision of structured data to build a more capable and accurate knowledge model."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yku71qTyDNq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3678e2e3",
        "outputId": "1e890fc5-ff05-415b-d685-d14561739def"
      },
      "source": [
        "# This is a hypothetical example. In a real scenario, you would use\n",
        "# more advanced techniques (like natural language processing) to extract\n",
        "# specific information reliably.\n",
        "\n",
        "# Let's assume the company name is mentioned early in the text\n",
        "# We'll just take a placeholder for demonstration\n",
        "company_name = \"Example Logistics Company\"\n",
        "\n",
        "# Let's assume we identify a key service mentioned\n",
        "key_service = \"US Import/Export Drayage\"\n",
        "\n",
        "structured_data_example = {\n",
        "    \"company_name\": company_name,\n",
        "    \"key_service_highlight\": key_service,\n",
        "    \"source_document\": \"company insight.pdf\"\n",
        "}\n",
        "\n",
        "import json\n",
        "\n",
        "# Convert the dictionary to a JSON string\n",
        "structured_data_json = json.dumps(structured_data_example, indent=4)\n",
        "\n",
        "print(structured_data_json)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"company_name\": \"Example Logistics Company\",\n",
            "    \"key_service_highlight\": \"US Import/Export Drayage\",\n",
            "    \"source_document\": \"company insight.pdf\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f995cb"
      },
      "source": [
        "### Creating Simple Structured Data\n",
        "\n",
        "Now, let's imagine we want to extract a specific piece of information from the text, like the company's name or a key service offered, and represent it in a structured format like a Python dictionary (which can be easily converted to JSON). This is a simplified example of how we would create structured data from unstructured text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GENERATE WITH AI\n"
      ],
      "metadata": {
        "id": "rUvgkLo3Dl86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "524820cf",
        "outputId": "1a6c28ae-79ea-4a70-ae49-0c3773f525c6"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Performs basic cleaning on text data.\"\"\"\n",
        "    # Remove excessive whitespace and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # You might add more cleaning steps here, like removing special characters\n",
        "    # text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Convert to lowercase (optional, depending on the model)\n",
        "    # text = text.lower()\n",
        "    return text\n",
        "\n",
        "cleaned_company_insight_text = clean_text(company_insight_text)\n",
        "\n",
        "# Print the first 500 characters of the cleaned text\n",
        "print(cleaned_company_insight_text[:500])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'company_insight_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2175373592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcleaned_company_insight_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_insight_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Print the first 500 characters of the cleaned text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'company_insight_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edead124"
      },
      "source": [
        "### Cleaning Unstructured Text\n",
        "\n",
        "First, let's perform some basic cleaning on the extracted text from the PDF. This typically involves removing unnecessary whitespace, special characters, and potentially converting text to lowercase for consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "4032945d",
        "outputId": "12f15073-ba6d-4806-adbf-8189beb06e20"
      },
      "source": [
        "import gemma.jax as gjax\n",
        "\n",
        "model, params = gjax.GemmaForCausalLM.from_pretrained(\n",
        "    'gemma_3b_it',\n",
        "    # device_sharding=jax.sharding.PartitionSpec('dp', 'sp'), # Optional: for distributed training\n",
        "    # param_sharding=jax.sharding.PartitionSpec('dp', 'sp'),  # Optional: for distributed training\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gemma.jax'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3029555774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model, params = gjax.GemmaForCausalLM.from_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'gemma_3b_it'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# device_sharding=jax.sharding.PartitionSpec('dp', 'sp'), # Optional: for distributed training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gemma.jax'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "2a4d3b62",
        "outputId": "cc8b1855-db40-4065-cf22-0af6914a7709"
      },
      "source": [
        "model = gm.nn.Gemma3_4B()\n",
        "\n",
        "params = gm.ckpts.load_params(params_path)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Metadata file does not exist: /tmp/tmpicuim7m9/gemma_3b_it/_CHECKPOINT_METADATA\n",
            "WARNING:absl:Failed to get item metadata from directory /tmp/tmpicuim7m9/gemma_3b_it. Either it was not present in the checkpoint, or the handler does not support it.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'StepMetadata' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3814670710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemma3_4B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gemma/gm/ckpts/_checkpoint.py\u001b[0m in \u001b[0;36mload_params\u001b[0;34m(path, params, donate, text_only, sharding, quantize)\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mocp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardCheckpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m   \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_metadata_and_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CheckpointTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_dtype_struct_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gemma/gm/ckpts/_checkpoint.py\u001b[0m in \u001b[0;36m_get_metadata_and_path\u001b[0;34m(ckpt, path)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'StepMetadata' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "90edf70c",
        "outputId": "0b592ca4-8f12-499b-94db-232d82c489cd"
      },
      "source": [
        "from gemma import config as gm_config\n",
        "from gemma import transformer as gm_transformer\n",
        "from gemma import checkpoint as gm_checkpoint\n",
        "\n",
        "# Define the model configuration\n",
        "model_config = gm_config.GemmaConfig.from_json(\n",
        "    '/usr/local/lib/python3.12/dist-packages/gemma/configs/config_3b_it.json'\n",
        ")\n",
        "\n",
        "# Instantiate the model\n",
        "model = gm_transformer.Transformer(model_config)\n",
        "\n",
        "# Load the parameters using CheckpointManager\n",
        "checkpoint_manager = gm_checkpoint.CheckpointManager(\n",
        "    '/usr/local/lib/python3.12/dist-packages/gemma/checkpoints/gemma_3b_it' # This path might need adjustment\n",
        ")\n",
        "\n",
        "params = checkpoint_manager.restore('params')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'config' from 'gemma' (/usr/local/lib/python3.12/dist-packages/gemma/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-110717168.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgm_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgm_transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgm_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the model configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'config' from 'gemma' (/usr/local/lib/python3.12/dist-packages/gemma/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2ec1da4",
        "outputId": "85b8ebe6-95b6-49b4-ec98-c363339a77b3"
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "# Create a temporary directory to store checkpoints\n",
        "ckpt_dir = tempfile.mkdtemp()\n",
        "\n",
        "# Download the checkpoint files\n",
        "!gsutil -m cp -r gs://gemma-jax/checkpoints/gemma_3b_it {ckpt_dir}\n",
        "\n",
        "# Update the params path to the downloaded checkpoints\n",
        "params_path = os.path.join(ckpt_dir, 'gemma_3b_it')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BucketNotFoundException: 404 gs://gemma-jax bucket does not exist.\n",
            "CommandException: 1 file/object could not be transferred.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "iRjnbNREdugN"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-turns conversations\n",
        "\n",
        "The easiest way to chat with Gemma is to use the `gm.text.ChatSampler`. It hides the boilerplate of the conversation cache, as well as the `<start_of_turn>` / `<end_of_turn>` tokens used to format the conversation.\n",
        "\n",
        "Here, we set `multi_turn=True` when creating `gm.text.ChatSampler` (by default, the `ChatSampler` start a new conversation every time).\n",
        "\n",
        "In multi-turn mode, you can erase the previous conversation state, by passing `chatbot.chat(..., multi_turn=False)`."
      ]
    },
    {
      "metadata": {
        "id": "bGSE6aTYdxqt"
      },
      "cell_type": "code",
      "source": [
        "sampler = gm.text.ChatSampler(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    multi_turn=True,\n",
        "    print_stream=True,  # Print output as it is generated.\n",
        ")\n",
        "\n",
        "turn0 = sampler.chat('Share one methapore linking \"shadow\" and \"laughter\".')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "MIZqgSSRfmRS"
      },
      "cell_type": "code",
      "source": [
        "turn1 = sampler.chat('Expand it in a haiku.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uOQHd6eFd67c"
      },
      "cell_type": "markdown",
      "source": [
        "Note: By default (`multi_turn=False`), the conversation state is reset everytime, but you can still continue the previous conversation by passing `sampler.chat(..., multi_turn=True)`\n",
        "\n",
        "By default, greedy decoding is used. You can pass a custom `sampling=` method as kwargs:\n",
        "\n",
        "* `gm.text.Greedy()`: (default) Greedy decoding\n",
        "* `gm.text.RandomSampling()`: Simple random sampling with temperature, for more variety"
      ]
    },
    {
      "metadata": {
        "id": "AH0eWFWJaiNk"
      },
      "cell_type": "markdown",
      "source": [
        "## Sample a prompt\n",
        "\n",
        "For more control, we also provide a `gm.text.Sampler` which still perform efficient sampling (with kv-caching, early stopping,...).\n",
        "\n",
        "Prompting the sampler require to correctly add format the prompt with the `<start_of_turn>` / `<end_of_turn>` tokens (see the custom token section doc on [tokenizer](https://gemma-llm.readthedocs.io/en/latest/tokenizer.html))."
      ]
    },
    {
      "metadata": {
        "id": "6R5J42EiZtkC"
      },
      "cell_type": "code",
      "source": [
        "sampler = gm.text.Sampler(\n",
        "    model=model,\n",
        "    params=params,\n",
        ")\n",
        "\n",
        "prompt = \"\"\"<start_of_turn>user\n",
        "Give me a list of inspirational quotes.<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "out = sampler.sample(prompt, max_new_tokens=1000)\n",
        "print(out)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "TPf01271RuTs"
      },
      "cell_type": "markdown",
      "source": [
        "## Use the model directly\n",
        "\n",
        "Here's an example of predicting a single token, directly calling the model.\n",
        "\n",
        "The model input expectes encoded tokens. For this, we first need to encode the prompt with our tokenizer. See our [tokenizer](https://gemma-llm.readthedocs.io/en/latest/tokenizer.html) documentation for more information on using the tokenizer."
      ]
    },
    {
      "metadata": {
        "id": "mvCAQCDXZ0D3"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = gm.text.Gemma3Tokenizer()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "kH534DHohG67"
      },
      "cell_type": "markdown",
      "source": [
        "Note: When encoding the prompt, don't forget to add the beginning-of-string token with `add_bos=True`. All prompts feed to the model should start by this token."
      ]
    },
    {
      "metadata": {
        "id": "T1GC0OPRhHGc"
      },
      "cell_type": "code",
      "source": [
        "prompt = tokenizer.encode('One word to describe Paris: \\n\\n', add_bos=True)\n",
        "prompt = jnp.asarray(prompt)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "hHqyFnskZ5SC"
      },
      "cell_type": "markdown",
      "source": [
        "We then can call the model, and get the predicted logits."
      ]
    },
    {
      "metadata": {
        "id": "G3Kbo9hgRuTt"
      },
      "cell_type": "code",
      "source": [
        "# Run the model\n",
        "out = model.apply(\n",
        "    {'params': params},\n",
        "    tokens=prompt,\n",
        "    return_last_only=True,  # Only predict the last token\n",
        ")\n",
        "\n",
        "\n",
        "# Sample a token from the predicted logits\n",
        "next_token = jax.random.categorical(\n",
        "    jax.random.key(1),\n",
        "    out.logits\n",
        ")\n",
        "tokenizer.decode(next_token)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "TNb88k3EF5gu"
      },
      "cell_type": "markdown",
      "source": [
        "You can also display the next token probability."
      ]
    },
    {
      "metadata": {
        "id": "mIkOdE9dF45s"
      },
      "cell_type": "code",
      "source": [
        "tokenizer.plot_logits(out.logits)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3yeUUP8Rh5mU"
      },
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "\n",
        "* See our [multimodal](https://gemma-llm.readthedocs.io/en/latest/multimodal.html) example to query the model with images.\n",
        "* See our [finetuning](https://gemma-llm.readthedocs.io/en/latest/finetuning.html) example to train Gemma on your custom task.\n",
        "* See our [tool use](https://gemma-llm.readthedocs.io/en/latest/tool_use.html) tutorial to extend Gemma with external tools.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}